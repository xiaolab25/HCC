{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "iW8rni8bZx4s",
   "metadata": {
    "id": "iW8rni8bZx4s"
   },
   "source": [
    "# Xenium analysis\n",
    "related to Figure.2 & sup fig.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21445d0",
   "metadata": {
    "id": "e21445d0"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "1. **Major cell-type annotation** (normalization → PCA → optional Harmony → neighbors/UMAP → Leiden → marker scoring → `obs[\"majortype\"]`).\n",
    "2. **Downstream spatial analysis**:\n",
    "   - optional merge of compartment-specific subtype annotations into `obs[\"subtype\"]`,\n",
    "   - cellular neighborhoods (CN) from local composition within a fixed radius,\n",
    "   - distance-to-target gradients and program scoring for a query population.\n",
    "\n",
    "## Required input files (relative paths; edit in `CONFIG`)\n",
    "\n",
    "- Main `AnnData` file (`.h5ad`) with:\n",
    "  - `obsm[\"spatial\"]`: 2D coordinates (x, y) in **microns**\n",
    "  - `obs[\"sample\"]`: sample/library identifier\n",
    "  - `obs[\"cell_id\"]`: cell identifier within sample (if missing, `obs_names` will be used)\n",
    "  - counts in `.X` or `.layers[\"counts\"]`\n",
    "\n",
    "- Optional subset `.h5ad` files providing `obs[\"subtype\"]` for selected compartments (used to populate/overwrite the main object's `obs[\"subtype\"]`).\n",
    "\n",
    "## Outputs (written to `results/xenium_integrated/`)\n",
    "\n",
    "- `adata/`:\n",
    "  - `xenium_integrated.h5ad` (major type + subtype + CN + distance bins + scores)\n",
    "- `annotation/`: cluster markers and major-type plots\n",
    "- `downstream/`: CN tables/plots, distance-bin tables/plots, program scores and ranked genes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc045284",
   "metadata": {
    "id": "bc045284"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# CONFIG (edit paths/keys here)\n",
    "# =========================\n",
    "CONFIG = {\n",
    "    # ---- Input ----\n",
    "    \"INPUT_H5AD\": Path(\"data/QC.no_SOX2OT_EEF1G.h5ad\"),\n",
    "\n",
    "    # Optional: compartment-specific subtype annotation files.\n",
    "    # These are merged into `obs[SUBTYPE_KEY]` by (sample, cell_id).\n",
    "    \"SUBSET_H5AD\": {\n",
    "        \"Hepato\": Path(\"data/Hepato_anno.h5ad\"),\n",
    "        \"CAF\": Path(\"data/Fibro_anno.h5ad\"),\n",
    "        \"Myeloid\": Path(\"data/Myeloid_anno.h5ad\"),\n",
    "        \"T\": Path(\"data/T_anno.h5ad\"),\n",
    "    },\n",
    "    \"SUBSET_PRECEDENCE\": [\"Hepato\", \"CAF\", \"Myeloid\", \"T\"],\n",
    "\n",
    "    # ---- Output root ----\n",
    "    \"OUTROOT\": Path(\"results/xenium_integrated\"),\n",
    "\n",
    "    # ---- Keys in AnnData ----\n",
    "    \"KEYS\": {\n",
    "        \"sample\": \"sample\",\n",
    "        \"cell_id\": \"cell_id\",\n",
    "        \"spatial\": \"spatial\",\n",
    "        \"majortype\": \"majortype\",\n",
    "        \"subtype\": \"subtype\",\n",
    "    },\n",
    "    \"GROUP_KEY\": \"group\",     # optional; ignored if missing\n",
    "\n",
    "    # ---- Reproducibility ----\n",
    "    \"SEED\": 0,\n",
    "    \"FIG_DPI\": 200,\n",
    "\n",
    "    # ---- Preprocessing / clustering ----\n",
    "    \"DROP_GENES\": [],  # ignored if absent\n",
    "    \"N_HVG\": 2500,\n",
    "    \"N_PCS\": 30,\n",
    "    \"N_NEIGHBORS\": 30,\n",
    "    \"LEIDEN_RES\": 1.2,\n",
    "\n",
    "    # Batch integration (Harmony) across samples if >1 sample present\n",
    "    \"INTEGRATION\": \"harmony\",  # {\"harmony\", \"none\"}\n",
    "    \"HARMONY_MAX_ITER\": 50,\n",
    "\n",
    "    # Differential expression per cluster\n",
    "    \"DE_METHOD\": \"wilcoxon\",\n",
    "\n",
    "    # ---- Example sample (for illustrative spatial plots and Xenium Explorer exports) ----\n",
    "    # If None, the first sample (sorted) will be used.\n",
    "    \"EXAMPLE_SAMPLE\": None,\n",
    "\n",
    "    # ---- Cellular neighborhoods (CN) ----\n",
    "    \"RUN_CN\": True,\n",
    "    \"CN\": {\n",
    "        \"radius_um\": 100.0,\n",
    "        \"n_clusters\": 11,\n",
    "        \"standardize_within_sample\": True,\n",
    "        \"key_added\": \"spatial_cn\",\n",
    "        \"cn_obs_key\": \"CN\",\n",
    "    },\n",
    "\n",
    "    # ---- Distance-to-target gradients (default: CAF → EMT front) ----\n",
    "    \"DIST\": {\n",
    "        \"target_labels\": [\"EMT_FRONT\", \"EMT+Invasion-high\"],\n",
    "        \"query_regex\": [r\"^iCAF_\", r\"^myCAF_\"],\n",
    "        \"distance_min_um\": 0.0,\n",
    "        \"distance_max_um\": 200.0,\n",
    "        \"bin_size_um\": 10.0,\n",
    "        \"bayes_alpha\": 0.5,\n",
    "    },\n",
    "\n",
    "    # ---- Gene/program gradients (query cells only) ----\n",
    "    \"GENE_GRADIENT\": {\n",
    "        \"genes\": [\"FAP\", \"CTHRC1\", \"SULF1\", \"PDCD1LG2\"],\n",
    "        \"signatures\": {\n",
    "            \"ImmuneReg\": [\n",
    "                \"IL6\",\"IL11\",\"LIF\",\"CXCL12\",\"CXCL14\",\"CXCL1\",\"CXCL2\",\"CXCL8\",\n",
    "                \"CCL2\",\"CCL7\",\"ICAM1\",\"PTGS2\",\"TNFAIP6\",\"SERPINE1\",\"HAS1\",\"HAS2\",\n",
    "                \"PDGFRA\",\"IGF1\",\"OSM\",\"PRG4\"\n",
    "            ],\n",
    "            \"ECM\": [\n",
    "                \"ACTA2\",\"TAGLN\",\"MYL9\",\"TPM2\",\n",
    "                \"COL1A1\",\"COL1A2\",\"COL3A1\",\"COL5A1\",\"COL5A2\",\"COL6A1\",\"COL6A2\",\"COL6A3\",\"COL11A1\",\"COL12A1\",\n",
    "                \"FN1\",\"POSTN\",\"SPARC\",\"THBS1\",\"THBS2\",\"LOX\",\"LOXL2\",\"PLOD1\",\"PLOD2\",\"PLOD3\",\"SERPINH1\",\n",
    "                \"MMP2\",\"MMP11\",\"MMP14\",\"ITGA11\",\"DDR2\",\"PDGFRB\",\"FBLN1\",\"FBLN2\",\"TNC\",\"ASPN\",\"LUM\",\"DCN\",\"FAP\"\n",
    "            ],\n",
    "            \"Antigen_presentation\": [\n",
    "                \"HLA-DRA\",\"HLA-DRB1\",\"HLA-DRB5\",\"HLA-DQA1\",\"HLA-DQB1\",\"HLA-DQA2\",\"HLA-DQB2\",\n",
    "                \"HLA-DPA1\",\"HLA-DPB1\",\"HLA-DMA\",\"HLA-DMB\",\"HLA-DOA\",\"HLA-DOB\",\"CD74\",\"CIITA\",\n",
    "                \"CTSS\",\"CTSL\",\"CTSB\",\"LGMN\",\"IFI30\",\"LAMP1\",\"LAMP2\",\"RFX5\",\"RFXAP\",\"RFXANK\"\n",
    "            ],\n",
    "            \"Quiescent\": [\n",
    "                \"LRAT\",\"RBP1\",\"RELN\",\"LHX2\",\"NGFR\",\"PPARG\",\"GFAP\",\"SYNM\",\"SYNM\",\"SYP\",\n",
    "                \"ALDH1A1\",\"ALDH1A2\",\"RDH10\",\"RARB\"\n",
    "            ],\n",
    "        },\n",
    "        \"score_ctrl_size\": 50,\n",
    "        \"score_n_bins\": 25,\n",
    "        \"min_cells_per_sample_bin\": 20,\n",
    "        \"smooth_min\": 0.6,\n",
    "        \"r2_min\": 0.3,\n",
    "        \"top_k\": 100,\n",
    "    },\n",
    "\n",
    "\n",
    "    # ---- scRNA ↔ Xenium subtype mapping via Jaccard (marker-set overlap) ----\n",
    "    # This computes subtype-to-subtype similarity between a single-cell reference and Xenium\n",
    "    # by Jaccard overlap of filtered marker-gene sets (see downstream JACCARD section).\n",
    "    \"JACCARD\": {\n",
    "        \"RUN\": True,\n",
    "\n",
    "        # Single-cell reference (AnnData with obs[SC_GROUPBY])\n",
    "        \"SC_H5AD\": Path(\"data/sc_reference.h5ad\"),\n",
    "        \"SC_GROUPBY\": \"subtype\",\n",
    "\n",
    "        # Xenium grouping key (usually same as CONFIG[\"KEYS\"][\"subtype\"])\n",
    "        \"XE_GROUPBY\": \"subtype\",\n",
    "\n",
    "        # Optional: restrict Xenium cells to specific major-type(s) before mapping\n",
    "        # Example: [\"CAF\"] or [\"Myeloid\"] depending on your majortype naming.\n",
    "        \"XE_MAJORTYPE_IN\": None,\n",
    "\n",
    "        # Optional: drop specific labels prior to DEG / Jaccard\n",
    "        \"DROP_SC_LABELS\": [],   \n",
    "        \"DROP_XE_LABELS\": [],   \n",
    "\n",
    "        # Normalization used for DE in this section (copies are made; original objects untouched)\n",
    "        \"TARGET_SUM\": 1e4,\n",
    "\n",
    "        # DEG method\n",
    "        \"METHOD\": \"wilcoxon\",\n",
    "\n",
    "        # Marker-set definition (filters applied on the DE table + in/out expression fraction)\n",
    "        \"N_TOP_SC\": 100,\n",
    "        \"N_TOP_XE\": 50,\n",
    "        \"MIN_PCT_EXPR\": 0.25,\n",
    "        \"MIN_FC\": 1.2,          # fold-change cutoff (approx; assumes log2FC in Scanpy output)\n",
    "        \"MAX_FDR\": 0.05,\n",
    "        \"MIN_DELTA_PCT\": 0.10,  # pct_in - pct_out\n",
    "        \"MAX_PCT_OUT\": None,    # optional, e.g. 0.2\n",
    "\n",
    "        # Remove common \"housekeeping\" prefixes from marker sets\n",
    "        \"DROP_PREFIX\": (\"MT-\", \"RPS\", \"RPL\"),\n",
    "\n",
    "        # Optional: drop overly shared genes across subtypes to reduce \"generic marker\" effects\n",
    "        # None disables; 0.7 means: drop genes appearing in >70% of subtypes\n",
    "        \"DROP_OVER_SHARED_MAX_FRAC\": 0.7,\n",
    "\n",
    "        # Mapping diagnostics\n",
    "        \"JACCARD_THR\": 0.05,\n",
    "        \"MIN_CELLS_PER_GROUP\": 3,\n",
    "\n",
    "        # Output name stem (saved under results/xenium_integrated/downstream/jaccard/)\n",
    "        \"OUT_STEM\": \"jaccard_sc_vs_xenium_subtype\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# Output directories\n",
    "# =========================\n",
    "OUTROOT = CONFIG[\"OUTROOT\"]\n",
    "DIRS = {\n",
    "    \"root\": OUTROOT,\n",
    "    \"adata\": OUTROOT / \"adata\",\n",
    "    \"annotation\": OUTROOT / \"annotation\",\n",
    "    \"annotation_fig\": OUTROOT / \"annotation\" / \"figures\",\n",
    "    \"annotation_tbl\": OUTROOT / \"annotation\" / \"tables\",\n",
    "    \"downstream\": OUTROOT / \"downstream\",\n",
    "    \"downstream_fig\": OUTROOT / \"downstream\" / \"figures\",\n",
    "    \"downstream_tbl\": OUTROOT / \"downstream\" / \"tables\",\n",
    "    \"xenium_explorer\": OUTROOT / \"downstream\" / \"xenium_explorer\",\n",
    "}\n",
    "\n",
    "for d in DIRS.values():\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[CONFIG] INPUT_H5AD={CONFIG['INPUT_H5AD']}\")\n",
    "print(f\"[CONFIG] OUTROOT={OUTROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b41a34",
   "metadata": {
    "id": "32b41a34"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Imports, seeds, versions\n",
    "# =========================\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import platform\n",
    "import warnings\n",
    "from importlib import metadata as importlib_metadata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Optional (used if present)\n",
    "try:\n",
    "    import torch\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "try:\n",
    "    import scvi\n",
    "except Exception:\n",
    "    scvi = None\n",
    "\n",
    "# Squidpy is only required if CN is enabled\n",
    "if bool(CONFIG[\"RUN_CN\"]):\n",
    "    import squidpy as sq\n",
    "\n",
    "from scipy.ndimage import (\n",
    "    gaussian_filter,\n",
    "    label as nd_label,\n",
    "    binary_fill_holes,\n",
    "    distance_transform_edt,\n",
    ")\n",
    "from skimage.measure import find_contours\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "SEED = int(CONFIG[\"SEED\"])\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "if torch is not None:\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    try:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if scvi is not None:\n",
    "    try:\n",
    "        scvi.settings.seed = SEED\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# -------------------------\n",
    "# Plot defaults (vector-friendly)\n",
    "# -------------------------\n",
    "mpl.rcParams.update(\n",
    "    {\n",
    "        \"pdf.fonttype\": 42,\n",
    "        \"ps.fonttype\": 42,\n",
    "        \"svg.fonttype\": \"none\",\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"legend.frameon\": False,\n",
    "        \"figure.dpi\": 120,\n",
    "    }\n",
    ")\n",
    "sns.set_style(\"ticks\")\n",
    "sc.settings.verbosity = 2\n",
    "sc.settings.set_figure_params(dpi=int(CONFIG[\"FIG_DPI\"]), frameon=False)\n",
    "\n",
    "# -------------------------\n",
    "# Print key versions\n",
    "# -------------------------\n",
    "def _v(pkg: str) -> str:\n",
    "    try:\n",
    "        return importlib_metadata.version(pkg)\n",
    "    except Exception:\n",
    "        return \"NA\"\n",
    "\n",
    "versions = {\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"scanpy\": _v(\"scanpy\"),\n",
    "    \"anndata\": _v(\"anndata\"),\n",
    "    \"numpy\": _v(\"numpy\"),\n",
    "    \"pandas\": _v(\"pandas\"),\n",
    "    \"scipy\": _v(\"scipy\"),\n",
    "    \"scikit-learn\": _v(\"scikit-learn\"),\n",
    "    \"matplotlib\": _v(\"matplotlib\"),\n",
    "    \"seaborn\": _v(\"seaborn\"),\n",
    "    \"squidpy\": _v(\"squidpy\"),\n",
    "    \"harmonypy\": _v(\"harmonypy\"),\n",
    "    \"scikit-image\": _v(\"scikit-image\"),\n",
    "    \"torch\": _v(\"torch\"),\n",
    "    \"scvi-tools\": _v(\"scvi-tools\"),\n",
    "}\n",
    "\n",
    "print(\"[VERSIONS]\")\n",
    "for k, v in versions.items():\n",
    "    print(f\"  {k:>12}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b75e7",
   "metadata": {
    "id": "dc7b75e7"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Helper functions\n",
    "# =========================\n",
    "\n",
    "from typing import Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Canonical marker sets for *major* cell types.\n",
    "# These are intended as a reproducible baseline and may be adjusted for the gene panel.\n",
    "MAJOR_MARKERS: Dict[str, List[str]] = {\n",
    "    \"T_cell\": [\"CD3D\", \"CD3E\", \"TRAC\", \"IL7R\", \"LTB\"],\n",
    "    \"NK_cell\": [\"NKG7\", \"GNLY\", \"PRF1\", \"GZMB\"],\n",
    "    \"B_cell\": [\"MS4A1\", \"CD79A\", \"CD74\", \"HLA-DRA\"],\n",
    "    \"Plasma_cell\": [\"MZB1\", \"JCHAIN\", \"XBP1\", \"SDC1\"],\n",
    "    \"Myeloid\": [\"LST1\", \"TYROBP\", \"FCER1G\", \"LYZ\"],\n",
    "    \"Dendritic\": [\"FCER1A\", \"CLEC10A\", \"ITGAX\", \"LILRA4\"],\n",
    "    \"Endothelial\": [\"PECAM1\", \"VWF\", \"KDR\", \"EMCN\"],\n",
    "    \"Fibroblast\": [\"COL1A1\", \"COL1A2\", \"DCN\", \"LUM\", \"COL3A1\"],\n",
    "    \"Pericyte_SMC\": [\"RGS5\", \"PDGFRB\", \"CSPG4\", \"ACTA2\", \"MCAM\"],\n",
    "    \"Epithelial\": [\"EPCAM\", \"KRT8\", \"KRT18\", \"KRT19\"],\n",
    "    \"Hepatocyte\": [\"ALB\", \"APOA1\", \"TTR\", \"FABP1\"],\n",
    "    \"Mast\": [\"TPSAB1\", \"TPSB2\", \"KIT\"],\n",
    "    \"Erythroid\": [\"HBB\", \"HBA1\", \"HBA2\"],\n",
    "}\n",
    "\n",
    "def _uppercase_lookup(var_names: Sequence[str]) -> Dict[str, str]:\n",
    "    \"\"\"Map uppercase gene symbols -> actual var_names entries (first occurrence wins).\"\"\"\n",
    "    out: Dict[str, str] = {}\n",
    "    for g in var_names:\n",
    "        gu = str(g).upper()\n",
    "        if gu not in out:\n",
    "            out[gu] = str(g)\n",
    "    return out\n",
    "\n",
    "def safe_score_genes(\n",
    "    adata: sc.AnnData,\n",
    "    gene_list: Sequence[str],\n",
    "    score_name: str,\n",
    "    *,\n",
    "    use_raw: bool = True,\n",
    "    min_genes: int = 2,\n",
    ") -> None:\n",
    "    \"\"\"Score a marker set with graceful handling of missing genes.\"\"\"\n",
    "    src = adata.raw if (use_raw and adata.raw is not None) else adata\n",
    "    lookup = _uppercase_lookup(src.var_names)\n",
    "    present = [lookup[g.upper()] for g in gene_list if g.upper() in lookup]\n",
    "    if len(present) < min_genes:\n",
    "        adata.obs[score_name] = np.nan\n",
    "        return\n",
    "    sc.tl.score_genes(adata, gene_list=present, score_name=score_name, use_raw=use_raw)\n",
    "\n",
    "def save_figure(fig: plt.Figure, stem: str) -> Tuple[Path, Path]:\n",
    "    \"\"\"Save a figure to both PDF and PNG under FIGDIR, and close it.\"\"\"\n",
    "    pdf = FIGDIR / f\"{stem}.pdf\"\n",
    "    png = FIGDIR / f\"{stem}.png\"\n",
    "    fig.savefig(pdf, bbox_inches=\"tight\")\n",
    "    fig.savefig(png, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return pdf, png\n",
    "\n",
    "def _load_subset_labels(path: Path, *, sample_key: str, cell_id_key: str, subtype_key: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a subset .h5ad and return a mapping table: (sample, cell_id) -> subtype.\"\"\"\n",
    "    sub = sc.read_h5ad(path)\n",
    "    obs = sub.obs.copy()\n",
    "\n",
    "    if sample_key not in obs:\n",
    "        raise KeyError(f\"{path}: obs[{sample_key!r}] not found.\")\n",
    "    if subtype_key not in obs:\n",
    "        raise KeyError(f\"{path}: obs[{subtype_key!r}] not found.\")\n",
    "\n",
    "    if cell_id_key in obs:\n",
    "        obs[cell_id_key] = obs[cell_id_key].astype(str)\n",
    "    else:\n",
    "        # Fall back to obs_names if the file does not store `cell_id` explicitly\n",
    "        obs[cell_id_key] = sub.obs_names.astype(str)\n",
    "\n",
    "    obs[sample_key] = obs[sample_key].astype(str)\n",
    "    obs[subtype_key] = obs[subtype_key].astype(str)\n",
    "\n",
    "    out = obs[[sample_key, cell_id_key, subtype_key]].dropna().drop_duplicates()\n",
    "    return out\n",
    "\n",
    "def merge_subtype_from_subsets(\n",
    "    adata: \"sc.AnnData\",\n",
    "    subset_paths: dict,\n",
    "    precedence: list,\n",
    "    *,\n",
    "    sample_key: str,\n",
    "    cell_id_key: str,\n",
    "    subtype_key: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Overwrite/construct `adata.obs[subtype_key]` by merging subtype annotations from subset files.\n",
    "\n",
    "    Precedence is applied in order: later sources overwrite earlier ones only for cells present in that subset file.\n",
    "    \"\"\"\n",
    "    if subtype_key not in adata.obs:\n",
    "        # Fallback: if a coarse label exists, use it; otherwise start empty.\n",
    "        if \"majortype\" in adata.obs:\n",
    "            adata.obs[subtype_key] = adata.obs[\"majortype\"].astype(str)\n",
    "        else:\n",
    "            adata.obs[subtype_key] = pd.NA\n",
    "\n",
    "    base = adata.obs[subtype_key].astype(\"string\")\n",
    "    key_series = adata.obs[sample_key].astype(str) + \"||\" + adata.obs[cell_id_key].astype(str)\n",
    "\n",
    "    for tag in precedence:\n",
    "        p = subset_paths.get(tag, None)\n",
    "        if p is None:\n",
    "            continue\n",
    "        if not Path(p).exists():\n",
    "            warnings.warn(f\"[WARN] Subset file not found, skipped: {p}\")\n",
    "            continue\n",
    "\n",
    "        df = _load_subset_labels(Path(p), sample_key=sample_key, cell_id_key=cell_id_key, subtype_key=subtype_key)\n",
    "        map_key = df[sample_key].astype(str) + \"||\" + df[cell_id_key].astype(str)\n",
    "        mapping = pd.Series(df[subtype_key].values, index=map_key).dropna()\n",
    "        mapped = key_series.map(mapping)\n",
    "        mask = mapped.notna()\n",
    "        if mask.sum() > 0:\n",
    "            base.loc[mask] = mapped.loc[mask].astype(\"string\")\n",
    "\n",
    "        print(f\"[MERGE] {tag}: updated {int(mask.sum())} cells from {p}\")\n",
    "\n",
    "    adata.obs[subtype_key] = pd.Categorical(base.astype(str))\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "# ============================================================\n",
    "# Core helpers\n",
    "# ============================================================\n",
    "def gene_1d(\n",
    "    adata: sc.AnnData,\n",
    "    gene: str,\n",
    "    *,\n",
    "    layer: Optional[str] = None,\n",
    "    use_raw: bool = False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Return a dense 1D vector for a gene.\n",
    "\n",
    "    - If use_raw=True: uses adata.raw (no layers).\n",
    "    - Else if layer is None: uses adata.X.\n",
    "    - Else: uses adata.layers[layer].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Shape (n_cells,), dtype float.\n",
    "    \"\"\"\n",
    "    if use_raw:\n",
    "        if adata.raw is None:\n",
    "            raise ValueError(\"use_raw=True but adata.raw is None.\")\n",
    "        if gene not in adata.raw.var_names:\n",
    "            raise KeyError(f\"Gene '{gene}' not found in adata.raw.var_names.\")\n",
    "        X = adata.raw[:, gene].X\n",
    "    else:\n",
    "        if gene not in adata.var_names:\n",
    "            raise KeyError(f\"Gene '{gene}' not found in adata.var_names.\")\n",
    "        if layer is None:\n",
    "            X = adata[:, gene].X\n",
    "        else:\n",
    "            if layer not in adata.layers:\n",
    "                raise KeyError(f\"Layer '{layer}' not found in adata.layers.\")\n",
    "            X = adata[:, gene].layers[layer]\n",
    "\n",
    "    if sparse.issparse(X):\n",
    "        return X.toarray().ravel().astype(float)\n",
    "    return np.asarray(X).ravel().astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb80188",
   "metadata": {
    "id": "6bb80188"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Load, preprocess, clustering, major-type annotation\n",
    "# =========================\n",
    "keys = CONFIG[\"KEYS\"]\n",
    "sample_key = keys[\"sample\"]\n",
    "cell_id_key = keys[\"cell_id\"]\n",
    "spatial_key = keys[\"spatial\"]\n",
    "majortype_key = keys[\"majortype\"]\n",
    "subtype_key = keys[\"subtype\"]\n",
    "\n",
    "ANN_FIGDIR = DIRS[\"annotation_fig\"]\n",
    "ANN_TBLDIR = DIRS[\"annotation_tbl\"]\n",
    "\n",
    "# save_figure() writes to the global FIGDIR\n",
    "FIGDIR = ANN_FIGDIR\n",
    "\n",
    "if not CONFIG[\"INPUT_H5AD\"].exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing input file: {CONFIG['INPUT_H5AD']}. \"\n",
    "        \"Update CONFIG['INPUT_H5AD'] or place the file under the expected path.\"\n",
    "    )\n",
    "\n",
    "adata = sc.read_h5ad(CONFIG[\"INPUT_H5AD\"])\n",
    "print(f\"[LOAD] AnnData: {adata.n_obs:,} cells × {adata.n_vars:,} genes\")\n",
    "\n",
    "# ---- Required fields for the full pipeline ----\n",
    "for k in [sample_key]:\n",
    "    if k not in adata.obs:\n",
    "        raise KeyError(f\"Required obs column missing: {k!r}\")\n",
    "if spatial_key not in adata.obsm:\n",
    "    raise KeyError(f\"Required obsm key missing: {spatial_key!r} (needed for downstream spatial analysis)\")\n",
    "\n",
    "# Ensure `cell_id` is available for robust cross-file joins\n",
    "if cell_id_key not in adata.obs:\n",
    "    adata.obs[cell_id_key] = adata.obs_names.astype(str)\n",
    "\n",
    "# Standardize key dtypes early\n",
    "adata.obs[sample_key] = adata.obs[sample_key].astype(str)\n",
    "adata.obs[cell_id_key] = adata.obs[cell_id_key].astype(str)\n",
    "\n",
    "# Optional gene removal (ignored if absent)\n",
    "drop = [g for g in CONFIG[\"DROP_GENES\"] if g in adata.var_names]\n",
    "if drop:\n",
    "    adata = adata[:, ~adata.var_names.isin(drop)].copy()\n",
    "    print(f\"[PREP] Dropped genes: {drop}\")\n",
    "\n",
    "# Ensure a counts layer is available\n",
    "if \"counts\" not in adata.layers:\n",
    "    adata.layers[\"counts\"] = adata.X.copy()\n",
    "\n",
    "# QC metrics (kept minimal; upstream QC/filtering is assumed)\n",
    "adata.var[\"mt\"] = adata.var_names.str.upper().str.startswith(\"MT-\")\n",
    "sc.pp.calculate_qc_metrics(adata, qc_vars=[\"mt\"], inplace=True)\n",
    "\n",
    "# Normalize + log1p (working matrix in .X)\n",
    "adata.X = adata.layers[\"counts\"].copy()\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "\n",
    "# Keep a log1p layer and preserve full-gene log1p as .raw\n",
    "adata.layers[\"log1p\"] = adata.X.copy()\n",
    "adata.raw = adata\n",
    "\n",
    "batch_key = sample_key if adata.obs[sample_key].nunique() > 1 else None\n",
    "group_key = CONFIG[\"GROUP_KEY\"] if (CONFIG[\"GROUP_KEY\"] in adata.obs.columns) else None\n",
    "\n",
    "# ------------------- HVGs → PCA → (Harmony) → UMAP → Leiden -------------------\n",
    "hvg_kwargs = dict(n_top_genes=int(CONFIG[\"N_HVG\"]), flavor=\"seurat_v3\", layer=\"counts\")\n",
    "if batch_key is not None:\n",
    "    hvg_kwargs[\"batch_key\"] = batch_key\n",
    "\n",
    "try:\n",
    "    sc.pp.highly_variable_genes(adata, **hvg_kwargs)\n",
    "except ImportError:\n",
    "    # seurat_v3 requires scikit-misc; fall back to a dependency-free HVG flavor\n",
    "    hvg_kwargs[\"flavor\"] = \"seurat\"\n",
    "    hvg_kwargs.pop(\"layer\", None)\n",
    "    sc.pp.highly_variable_genes(adata, **hvg_kwargs)\n",
    "\n",
    "adata = adata[:, adata.var[\"highly_variable\"]].copy()\n",
    "\n",
    "# Optional regression (only if columns exist)\n",
    "regress_keys = [k for k in [\"total_counts\", \"pct_counts_mt\"] if k in adata.obs.columns]\n",
    "if regress_keys:\n",
    "    sc.pp.regress_out(adata, keys=regress_keys)\n",
    "\n",
    "sc.pp.scale(adata, max_value=10)\n",
    "sc.tl.pca(adata, n_comps=int(CONFIG[\"N_PCS\"]), svd_solver=\"arpack\", random_state=SEED)\n",
    "\n",
    "use_rep = \"X_pca\"\n",
    "if (\n",
    "    str(CONFIG[\"INTEGRATION\"]).lower() == \"harmony\"\n",
    "    and batch_key is not None\n",
    "    and adata.obs[batch_key].nunique() > 1\n",
    "):\n",
    "    try:\n",
    "        sc.external.pp.harmony_integrate(\n",
    "            adata,\n",
    "            key=batch_key,\n",
    "            basis=\"X_pca\",\n",
    "            adjusted_basis=\"X_pca_harmony\",\n",
    "            max_iter_harmony=int(CONFIG[\"HARMONY_MAX_ITER\"]),\n",
    "        )\n",
    "        use_rep = \"X_pca_harmony\"\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"[WARN] Harmony integration failed ({type(e).__name__}: {e}). Falling back to unintegrated PCA.\")\n",
    "        use_rep = \"X_pca\"\n",
    "\n",
    "sc.pp.neighbors(\n",
    "    adata,\n",
    "    n_neighbors=int(CONFIG[\"N_NEIGHBORS\"]),\n",
    "    n_pcs=int(CONFIG[\"N_PCS\"]),\n",
    "    use_rep=use_rep,\n",
    "    random_state=SEED,\n",
    ")\n",
    "sc.tl.umap(adata, random_state=SEED)\n",
    "\n",
    "cluster_key = f\"leiden_r{CONFIG['LEIDEN_RES']}\"\n",
    "sc.tl.leiden(adata, resolution=float(CONFIG[\"LEIDEN_RES\"]), key_added=cluster_key, random_state=SEED)\n",
    "adata.obs[cluster_key] = adata.obs[cluster_key].astype(\"category\")\n",
    "print(f\"[CLUSTER] {adata.obs[cluster_key].nunique()} clusters in obs['{cluster_key}']\")\n",
    "\n",
    "# ------------------- DE per cluster (markers) -------------------\n",
    "sc.tl.rank_genes_groups(adata, groupby=cluster_key, method=str(CONFIG[\"DE_METHOD\"]), use_raw=True)\n",
    "markers_df = sc.get.rank_genes_groups_df(adata, group=None)\n",
    "markers_csv = ANN_TBLDIR / \"cluster_markers_all.csv\"\n",
    "markers_df.to_csv(markers_csv, index=False)\n",
    "# ------------------- Marker scoring (assist manual annotation) -------------------\n",
    "score_cols = []\n",
    "for ct, genes in MAJOR_MARKERS.items():\n",
    "    col = f\"score_{ct}\"\n",
    "    safe_score_genes(adata, genes, score_name=col, use_raw=True)\n",
    "    score_cols.append(col)\n",
    "\n",
    "# Aggregate marker scores per cluster (helps manual labeling)\n",
    "score_by_cluster_csv = ANN_TBLDIR / \"marker_scores_by_cluster.csv\"\n",
    "adata.obs[[cluster_key] + score_cols].groupby(cluster_key).mean().to_csv(score_by_cluster_csv)\n",
    "\n",
    "# Export a template for manual cluster -> majortype annotation\n",
    "cluster_anno_template_csv = ANN_TBLDIR / \"cluster_annotation_template.csv\"\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        cluster_key: adata.obs[cluster_key].cat.categories.astype(str),\n",
    "        majortype_key: \"\",\n",
    "    }\n",
    ").to_csv(cluster_anno_template_csv, index=False)\n",
    "\n",
    "# ------------------- Manual major-type annotation (NO automatic assignment) -------------------\n",
    "# Manual step:\n",
    "# 1) Inspect:\n",
    "#    - cluster markers table: cluster_markers_all.csv\n",
    "#    - dotplot of canonical markers by cluster: dotplot_major_markers_by_cluster.(pdf/png)\n",
    "#    - mean marker-score table by cluster: marker_scores_by_cluster.csv\n",
    "# 2) Fill CLUSTER2MAJORTYPE mapping below (cluster labels are strings), or\n",
    "#    edit cluster_annotation_template.csv and load it back (optional).\n",
    "\n",
    "CLUSTER2MAJORTYPE = {\n",
    "    # \"0\": \"T\",\n",
    "    # \"1\": \"Macro\",\n",
    "    # \"2\": \"Hepato\",\n",
    "    # ...\n",
    "}\n",
    "\n",
    "adata.obs[majortype_key] = (\n",
    "    adata.obs[cluster_key].astype(str).map(CLUSTER2MAJORTYPE).fillna(\"Unassigned\")\n",
    ").astype(\"category\")\n",
    "\n",
    "# Summary tables based on current majortype (may contain many 'Unassigned' until mapping is filled)\n",
    "ct_counts_csv = ANN_TBLDIR / \"majortype_counts.csv\"\n",
    "adata.obs[majortype_key].value_counts().rename(\"n_cells\").to_csv(ct_counts_csv)\n",
    "\n",
    "cluster_by_ct_csv = ANN_TBLDIR / \"cluster_by_majortype.csv\"\n",
    "pd.crosstab(adata.obs[cluster_key], adata.obs[majortype_key]).to_csv(cluster_by_ct_csv)\n",
    "\n",
    "# ------------------- Key figures -------------------\n",
    "fig = sc.pl.umap(adata, color=[cluster_key], return_fig=True)\n",
    "save_figure(fig, \"umap_clusters\")\n",
    "\n",
    "fig = sc.pl.umap(\n",
    "    adata,\n",
    "    color=[majortype_key]\n",
    "    + ([batch_key] if batch_key is not None else [])\n",
    "    + ([group_key] if group_key is not None else []),\n",
    "    return_fig=True,\n",
    ")\n",
    "save_figure(fig, \"umap_majortype\")\n",
    "\n",
    "# Dotplot of canonical markers grouped by cluster (for manual annotation)\n",
    "raw_var = adata.raw.var_names if adata.raw is not None else adata.var_names\n",
    "lookup = _uppercase_lookup(raw_var)\n",
    "markers_present = {}\n",
    "for ct, genes in MAJOR_MARKERS.items():\n",
    "    present = [lookup[g.upper()] for g in genes if g.upper() in lookup]\n",
    "    if present:\n",
    "        markers_present[ct] = present\n",
    "\n",
    "if markers_present:\n",
    "    dp = sc.pl.dotplot(\n",
    "        adata,\n",
    "        markers_present,\n",
    "        groupby=cluster_key,\n",
    "        dendrogram=True,\n",
    "        standard_scale=\"var\",\n",
    "        return_fig=True,\n",
    "    )\n",
    "    try:\n",
    "        dp.savefig(ANN_FIGDIR / \"dotplot_major_markers_by_cluster.pdf\")\n",
    "        dp.savefig(ANN_FIGDIR / \"dotplot_major_markers_by_cluster.png\")\n",
    "    except Exception:\n",
    "        ax = dp.get_axes()[\"mainplot_ax\"]\n",
    "        ax.figure.savefig(ANN_FIGDIR / \"dotplot_major_markers_by_cluster.pdf\", bbox_inches=\"tight\")\n",
    "        ax.figure.savefig(ANN_FIGDIR / \"dotplot_major_markers_by_cluster.png\", bbox_inches=\"tight\")\n",
    "    plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d71d31a",
   "metadata": {
    "id": "1d71d31a"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Subtype merge, cellular neighborhoods, distance gradients\n",
    "# =========================\n",
    "FIG_DIR = DIRS[\"downstream_fig\"]\n",
    "TAB_DIR = DIRS[\"downstream_tbl\"]\n",
    "XE_DIR  = DIRS[\"xenium_explorer\"]\n",
    "\n",
    "# ---- Minimal sanity checks ----\n",
    "for k in (sample_key, cell_id_key, majortype_key):\n",
    "    if k not in adata.obs:\n",
    "        raise KeyError(f\"Required obs column missing: {k!r}\")\n",
    "if spatial_key not in adata.obsm:\n",
    "    raise KeyError(f\"Required obsm key missing: {spatial_key!r}\")\n",
    "\n",
    "adata.obs[sample_key] = adata.obs[sample_key].astype(str)\n",
    "adata.obs[cell_id_key] = adata.obs[cell_id_key].astype(str)\n",
    "\n",
    "# =========================\n",
    "# Construct / merge `subtype`\n",
    "# =========================\n",
    "merge_subtype_from_subsets(\n",
    "    adata,\n",
    "    subset_paths=CONFIG[\"SUBSET_H5AD\"],\n",
    "    precedence=CONFIG[\"SUBSET_PRECEDENCE\"],\n",
    "    sample_key=sample_key,\n",
    "    cell_id_key=cell_id_key,\n",
    "    subtype_key=subtype_key,\n",
    ")\n",
    "\n",
    "# Example sample for illustrative plots / Xenium Explorer export\n",
    "if CONFIG[\"EXAMPLE_SAMPLE\"] is None:\n",
    "    example_sample = sorted(adata.obs[sample_key].astype(str).unique())[0]\n",
    "else:\n",
    "    example_sample = str(CONFIG[\"EXAMPLE_SAMPLE\"])\n",
    "print(f\"[DATA] example sample: {example_sample}\")\n",
    "\n",
    "# Xenium Explorer grouping CSV (one example sample)\n",
    "xe_csv = XE_DIR / f\"{example_sample}_groups_subtype.csv\"\n",
    "(\n",
    "    adata.obs.loc[adata.obs[sample_key] == example_sample, [cell_id_key, subtype_key]]\n",
    "    .rename(columns={cell_id_key: \"cell_id\", subtype_key: \"group\"})\n",
    "    .to_csv(xe_csv, index=False)\n",
    ")\n",
    "print(f\"[OK] Xenium Explorer groups CSV: {xe_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0191af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Jaccard: scRNA subtype ↔ Xenium subtype (marker-set overlap)\n",
    "# =========================\n",
    "# Core logic (same as your example):\n",
    "# 1) DE per subtype in scRNA and Xenium (wilcoxon)\n",
    "# 2) Filter markers by (pct_in, FC, FDR, delta_pct, optional pct_out)\n",
    "# 3) Jaccard overlap of marker sets -> similarity matrix\n",
    "# 4) Export matrix + best matches (row-wise best + Hungarian 1-1)\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "jcfg = CONFIG.get(\"JACCARD\", {})\n",
    "if not bool(jcfg.get(\"RUN\", False)):\n",
    "    print(\"[JACCARD] Skipped (CONFIG['JACCARD']['RUN']=False)\")\n",
    "else:\n",
    "    sc_path = Path(jcfg.get(\"SC_H5AD\", \"\"))\n",
    "    if not sc_path.exists():\n",
    "        warnings.warn(f\"[WARN] SC_H5AD not found: {sc_path}. Skip Jaccard mapping.\")\n",
    "    else:\n",
    "        # -------------------------\n",
    "        # 0) Load scRNA reference\n",
    "        # -------------------------\n",
    "        ad_sc = sc.read_h5ad(sc_path)\n",
    "        sc_groupby = str(jcfg.get(\"SC_GROUPBY\", \"subtype\"))\n",
    "        xe_groupby = str(jcfg.get(\"XE_GROUPBY\", subtype_key))\n",
    "\n",
    "        if sc_groupby not in ad_sc.obs:\n",
    "            raise KeyError(f\"[JACCARD] scRNA obs[{sc_groupby!r}] not found in: {sc_path}\")\n",
    "        if xe_groupby not in adata.obs:\n",
    "            raise KeyError(f\"[JACCARD] Xenium obs[{xe_groupby!r}] not found in main adata.\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 1) Subset Xenium cells (optional majortype filter)\n",
    "        # -------------------------\n",
    "        xe_mask = np.ones(adata.n_obs, dtype=bool)\n",
    "\n",
    "        mt_in = jcfg.get(\"XE_MAJORTYPE_IN\", None)\n",
    "        if mt_in is not None:\n",
    "            if majortype_key not in adata.obs:\n",
    "                raise KeyError(f\"[JACCARD] majortype key {majortype_key!r} not in adata.obs; cannot apply XE_MAJORTYPE_IN.\")\n",
    "            mt_in = [str(x) for x in mt_in]\n",
    "            xe_mask &= adata.obs[majortype_key].astype(str).isin(mt_in).to_numpy()\n",
    "\n",
    "        # drop NA labels\n",
    "        xe_mask &= adata.obs[xe_groupby].notna().to_numpy()\n",
    "\n",
    "        ad_xe = adata[xe_mask].copy()\n",
    "        print(f\"[JACCARD] Xenium subset: {ad_xe.n_obs:,} cells (XE_MAJORTYPE_IN={mt_in})\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 2) Optional drop labels\n",
    "        # -------------------------\n",
    "        drop_sc = set(map(str, jcfg.get(\"DROP_SC_LABELS\", [])))\n",
    "        drop_xe = set(map(str, jcfg.get(\"DROP_XE_LABELS\", [])))\n",
    "\n",
    "        if drop_sc:\n",
    "            ad_sc = ad_sc[~ad_sc.obs[sc_groupby].astype(str).isin(drop_sc)].copy()\n",
    "        if drop_xe:\n",
    "            ad_xe = ad_xe[~ad_xe.obs[xe_groupby].astype(str).isin(drop_xe)].copy()\n",
    "\n",
    "        # remove tiny groups (Scanpy may fail if group size is too small)\n",
    "        min_cells = int(jcfg.get(\"MIN_CELLS_PER_GROUP\", 3))\n",
    "        for _ad, _k, _tag in [(ad_sc, sc_groupby, \"scRNA\"), (ad_xe, xe_groupby, \"Xenium\")]:\n",
    "            vc = _ad.obs[_k].astype(str).value_counts()\n",
    "            bad = vc[vc < min_cells].index.tolist()\n",
    "            if bad:\n",
    "                warnings.warn(f\"[JACCARD] {_tag}: drop {len(bad)} groups with <{min_cells} cells: {bad[:12]}{'...' if len(bad)>12 else ''}\")\n",
    "                _ad._inplace_subset_obs(~_ad.obs[_k].astype(str).isin(bad))\n",
    "            # clean categories\n",
    "            _ad.obs[_k] = _ad.obs[_k].astype(\"category\")\n",
    "            _ad.obs[_k] = _ad.obs[_k].cat.remove_unused_categories()\n",
    "\n",
    "        print(\"[JACCARD] scRNA subtype counts:\")\n",
    "        print(ad_sc.obs[sc_groupby].value_counts())\n",
    "        print(\"[JACCARD] Xenium subtype counts:\")\n",
    "        print(ad_xe.obs[xe_groupby].value_counts())\n",
    "\n",
    "        # -------------------------\n",
    "        # 3) Prepare log-normalized copies (do NOT modify originals)\n",
    "        # -------------------------\n",
    "        def _prep_log1p(a: sc.AnnData, target_sum: float = 1e4) -> sc.AnnData:\n",
    "            a = a.copy()\n",
    "            if \"counts\" not in a.layers:\n",
    "                a.layers[\"counts\"] = a.X.copy()\n",
    "            a.X = a.layers[\"counts\"].copy()\n",
    "            sc.pp.normalize_total(a, target_sum=float(target_sum))\n",
    "            sc.pp.log1p(a)\n",
    "            a.raw = a\n",
    "            return a\n",
    "\n",
    "        target_sum = float(jcfg.get(\"TARGET_SUM\", 1e4))\n",
    "        ad_sc2 = _prep_log1p(ad_sc, target_sum=target_sum)\n",
    "        ad_xe2 = _prep_log1p(ad_xe, target_sum=target_sum)\n",
    "\n",
    "        # -------------------------\n",
    "        # 4) Gene intersection (by name)\n",
    "        # -------------------------\n",
    "        genes_sc = ad_sc2.raw.var_names if ad_sc2.raw is not None else ad_sc2.var_names\n",
    "        genes_xe = ad_xe2.raw.var_names if ad_xe2.raw is not None else ad_xe2.var_names\n",
    "        common_genes = genes_sc.intersection(genes_xe)\n",
    "        print(f\"[JACCARD] common genes: {len(common_genes):,}\")\n",
    "        if len(common_genes) == 0:\n",
    "            raise ValueError(\"[JACCARD] No common genes between scRNA and Xenium; check gene naming (symbol vs Ensembl).\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 5) Rank genes by subtype (store under dedicated keys to avoid overwriting other DE results)\n",
    "        # -------------------------\n",
    "        key_sc = \"rg_sc_subtype\"\n",
    "        key_xe = \"rg_xe_subtype\"\n",
    "        method = str(jcfg.get(\"METHOD\", \"wilcoxon\"))\n",
    "\n",
    "        sc.tl.rank_genes_groups(ad_sc2, groupby=sc_groupby, method=method, use_raw=True, key_added=key_sc)\n",
    "        sc.tl.rank_genes_groups(ad_xe2, groupby=xe_groupby, method=method, use_raw=True, key_added=key_xe)\n",
    "\n",
    "        # -------------------------\n",
    "        # 6) Marker sets -> Jaccard matrix\n",
    "        # -------------------------\n",
    "        import math\n",
    "        from collections import Counter\n",
    "\n",
    "        def _as_upper_set(xs):\n",
    "            return set([str(x).upper() for x in xs])\n",
    "\n",
    "        def get_marker_sets_from_rank(\n",
    "            adata: sc.AnnData,\n",
    "            groupby: str,\n",
    "            *,\n",
    "            key: str,\n",
    "            n_top: int,\n",
    "            restrict_genes: pd.Index,\n",
    "            min_pct_expr: float,\n",
    "            min_fc: float,\n",
    "            max_fdr: float,\n",
    "            min_delta_pct: float,\n",
    "            max_pct_out: float | None,\n",
    "            drop_prefix: tuple[str, ...] = (\"MT-\", \"RPS\", \"RPL\"),\n",
    "        ) -> dict[str, set[str]]:\n",
    "            # Use RAW for pct calculations (raw uses log1p normalized: log1p(0)=0, so >0 is ok)\n",
    "            if adata.raw is not None:\n",
    "                X_use = adata.raw.X\n",
    "                var_names = pd.Index(adata.raw.var_names.astype(str))\n",
    "            else:\n",
    "                X_use = adata.X\n",
    "                var_names = pd.Index(adata.var_names.astype(str))\n",
    "\n",
    "            # Uppercase mapping\n",
    "            var_upper = var_names.str.upper()\n",
    "            upper_to_name = dict(zip(var_upper, var_names))\n",
    "            rg = _as_upper_set(restrict_genes)\n",
    "\n",
    "            # group list\n",
    "            groups = adata.obs[groupby].astype(str).unique().tolist()\n",
    "            out: dict[str, set[str]] = {}\n",
    "\n",
    "            # log2FC threshold\n",
    "            logfc_thr = math.log2(float(min_fc)) if float(min_fc) > 0 else -np.inf\n",
    "\n",
    "            for g in groups:\n",
    "                df = sc.get.rank_genes_groups_df(adata, group=g, key=key).copy()\n",
    "                if df.empty:\n",
    "                    out[str(g)] = set()\n",
    "                    continue\n",
    "\n",
    "                df[\"NAME_UPPER\"] = df[\"names\"].astype(str).str.upper()\n",
    "                df = df.set_index(\"NAME_UPPER\", drop=False)\n",
    "\n",
    "                # pct in/out\n",
    "                mask_in = (adata.obs[groupby].astype(str).values == str(g))\n",
    "                n_in = int(mask_in.sum())\n",
    "                n_out = int((~mask_in).sum())\n",
    "                if n_in == 0 or n_out == 0:\n",
    "                    out[str(g)] = set()\n",
    "                    continue\n",
    "\n",
    "                if sparse.issparse(X_use):\n",
    "                    pct_in = np.asarray(X_use[mask_in].getnnz(axis=0)).ravel() / n_in\n",
    "                    pct_out = np.asarray(X_use[~mask_in].getnnz(axis=0)).ravel() / n_out\n",
    "                else:\n",
    "                    pct_in = (X_use[mask_in] > 0).mean(axis=0)\n",
    "                    pct_out = (X_use[~mask_in] > 0).mean(axis=0)\n",
    "\n",
    "                pct_in = pd.Series(np.asarray(pct_in).ravel(), index=var_upper)\n",
    "                pct_out = pd.Series(np.asarray(pct_out).ravel(), index=var_upper)\n",
    "\n",
    "                # align to df\n",
    "                common = df.index.intersection(pct_in.index)\n",
    "                df = df.loc[common].copy()\n",
    "                df[\"pct_expr\"] = pct_in.loc[common].values\n",
    "                df[\"pct_out\"] = pct_out.loc[common].values\n",
    "                df[\"delta_pct\"] = df[\"pct_expr\"] - df[\"pct_out\"]\n",
    "\n",
    "                # columns existence\n",
    "                if \"logfoldchanges\" not in df.columns:\n",
    "                    df[\"logfoldchanges\"] = np.inf\n",
    "                if \"pvals_adj\" not in df.columns:\n",
    "                    df[\"pvals_adj\"] = 0.0\n",
    "\n",
    "                # filter\n",
    "                cond = (\n",
    "                    (df[\"pct_expr\"] >= float(min_pct_expr)) &\n",
    "                    (df[\"logfoldchanges\"] >= float(logfc_thr)) &\n",
    "                    (df[\"pvals_adj\"] <= float(max_fdr)) &\n",
    "                    (df[\"delta_pct\"] >= float(min_delta_pct))\n",
    "                )\n",
    "                if max_pct_out is not None:\n",
    "                    cond &= (df[\"pct_out\"] <= float(max_pct_out))\n",
    "\n",
    "                df = df.loc[cond].copy()\n",
    "\n",
    "                # drop prefixes\n",
    "                if drop_prefix:\n",
    "                    badmask = df.index.to_series().str.startswith(tuple([p.upper() for p in drop_prefix]))\n",
    "                    df = df.loc[~badmask]\n",
    "\n",
    "                # restrict to common genes (UPPER space)\n",
    "                df = df.loc[df.index.isin(rg)]\n",
    "\n",
    "                # take top n\n",
    "                df = df.sort_values(\"scores\", ascending=False)\n",
    "                top_upper = df.index.tolist()[: int(n_top)]\n",
    "                top_names = [upper_to_name.get(u, u) for u in top_upper]\n",
    "                out[str(g)] = set(map(str, top_names))\n",
    "\n",
    "            return out\n",
    "\n",
    "        def drop_over_shared_genes(marker_sets: dict[str, set[str]], max_frac: float = 0.7):\n",
    "            groups = list(marker_sets.keys())\n",
    "            n = len(groups)\n",
    "            cnt = Counter()\n",
    "            for g in groups:\n",
    "                cnt.update(marker_sets[g])\n",
    "            bad = {gene for gene, c in cnt.items() if (c / n) > float(max_frac)}\n",
    "            new = {g: set([x for x in marker_sets[g] if x not in bad]) for g in groups}\n",
    "            return new, bad\n",
    "\n",
    "        def jaccard(a: set[str], b: set[str]) -> float:\n",
    "            if len(a) == 0 and len(b) == 0:\n",
    "                return np.nan\n",
    "            if len(a) == 0 or len(b) == 0:\n",
    "                return 0.0\n",
    "            return len(a & b) / len(a | b)\n",
    "\n",
    "        def jaccard_matrix(markerA: dict[str, set[str]], markerB: dict[str, set[str]]) -> pd.DataFrame:\n",
    "            rows = list(markerA.keys())\n",
    "            cols = list(markerB.keys())\n",
    "            mat = np.zeros((len(rows), len(cols)), dtype=float)\n",
    "            for i, r in enumerate(rows):\n",
    "                for j, c in enumerate(cols):\n",
    "                    mat[i, j] = jaccard(markerA[r], markerB[c])\n",
    "            return pd.DataFrame(mat, index=rows, columns=cols)\n",
    "\n",
    "        m_sc = get_marker_sets_from_rank(\n",
    "            ad_sc2, groupby=sc_groupby, key=key_sc,\n",
    "            n_top=int(jcfg.get(\"N_TOP_SC\", 100)),\n",
    "            restrict_genes=common_genes,\n",
    "            min_pct_expr=float(jcfg.get(\"MIN_PCT_EXPR\", 0.25)),\n",
    "            min_fc=float(jcfg.get(\"MIN_FC\", 1.2)),\n",
    "            max_fdr=float(jcfg.get(\"MAX_FDR\", 0.05)),\n",
    "            min_delta_pct=float(jcfg.get(\"MIN_DELTA_PCT\", 0.10)),\n",
    "            max_pct_out=jcfg.get(\"MAX_PCT_OUT\", None),\n",
    "            drop_prefix=tuple(jcfg.get(\"DROP_PREFIX\", (\"MT-\", \"RPS\", \"RPL\"))),\n",
    "        )\n",
    "        m_xe = get_marker_sets_from_rank(\n",
    "            ad_xe2, groupby=xe_groupby, key=key_xe,\n",
    "            n_top=int(jcfg.get(\"N_TOP_XE\", 50)),\n",
    "            restrict_genes=common_genes,\n",
    "            min_pct_expr=float(jcfg.get(\"MIN_PCT_EXPR\", 0.25)),\n",
    "            min_fc=float(jcfg.get(\"MIN_FC\", 1.2)),\n",
    "            max_fdr=float(jcfg.get(\"MAX_FDR\", 0.05)),\n",
    "            min_delta_pct=float(jcfg.get(\"MIN_DELTA_PCT\", 0.10)),\n",
    "            max_pct_out=jcfg.get(\"MAX_PCT_OUT\", None),\n",
    "            drop_prefix=tuple(jcfg.get(\"DROP_PREFIX\", (\"MT-\", \"RPS\", \"RPL\"))),\n",
    "        )\n",
    "\n",
    "        drop_max_frac = jcfg.get(\"DROP_OVER_SHARED_MAX_FRAC\", None)\n",
    "        if drop_max_frac is not None:\n",
    "            m_sc, bad_sc = drop_over_shared_genes(m_sc, max_frac=float(drop_max_frac))\n",
    "            m_xe, bad_xe = drop_over_shared_genes(m_xe, max_frac=float(drop_max_frac))\n",
    "            print(f\"[JACCARD] dropped shared genes: sc={len(bad_sc)}, xenium={len(bad_xe)} (max_frac={drop_max_frac})\")\n",
    "\n",
    "        J = jaccard_matrix(m_sc, m_xe)\n",
    "\n",
    "        # -------------------------\n",
    "        # 7) Save outputs\n",
    "        # -------------------------\n",
    "        outstem = str(jcfg.get(\"OUT_STEM\", \"jaccard_sc_vs_xenium_subtype\"))\n",
    "        JROOT = DIRS[\"downstream\"] / \"jaccard\"\n",
    "        J_FIG = JROOT / \"figures\"\n",
    "        J_TBL = JROOT / \"tables\"\n",
    "        J_FIG.mkdir(parents=True, exist_ok=True)\n",
    "        J_TBL.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        j_csv = J_TBL / f\"{outstem}.matrix.csv\"\n",
    "        J.to_csv(j_csv)\n",
    "        print(f\"[OK] Jaccard matrix: {j_csv}\")\n",
    "\n",
    "        # best match per sc subtype (row-wise)\n",
    "        best = pd.DataFrame({\n",
    "            \"sc_subtype\": J.index.astype(str),\n",
    "            \"best_xenium_subtype\": J.idxmax(axis=1).astype(str).values,\n",
    "            \"jaccard\": J.max(axis=1).astype(float).values,\n",
    "        })\n",
    "        # margin top1-top2\n",
    "        X = np.nan_to_num(J.values.astype(float), nan=0.0)\n",
    "        row_sorted = np.sort(X, axis=1)\n",
    "        top2 = row_sorted[:, -2] if X.shape[1] >= 2 else np.zeros(X.shape[0])\n",
    "        best[\"margin_top1_top2\"] = best[\"jaccard\"].values - top2\n",
    "        best = best.sort_values([\"jaccard\", \"margin_top1_top2\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "        best_csv = J_TBL / f\"{outstem}.best_by_sc_subtype.csv\"\n",
    "        best.to_csv(best_csv, index=False)\n",
    "        print(f\"[OK] best-by-row: {best_csv}\")\n",
    "\n",
    "        # Hungarian 1-to-1 matching\n",
    "        cost = 1.0 - X\n",
    "        row_ind, col_ind = linear_sum_assignment(cost)\n",
    "        match_1to1 = pd.DataFrame({\n",
    "            \"sc_subtype\": J.index[row_ind].astype(str),\n",
    "            \"xenium_subtype\": J.columns[col_ind].astype(str),\n",
    "            \"jaccard\": X[row_ind, col_ind],\n",
    "        }).sort_values(\"jaccard\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "        match_csv = J_TBL / f\"{outstem}.match_1to1_hungarian.csv\"\n",
    "        match_1to1.to_csv(match_csv, index=False)\n",
    "        print(f\"[OK] hungarian 1-to-1: {match_csv}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 8) Heatmap (clustered order, no dendrogram)\n",
    "        # -------------------------\n",
    "        X0 = np.nan_to_num(J.values.astype(float), nan=0.0)\n",
    "        row_dist = pdist(X0, metric=\"correlation\")\n",
    "        col_dist = pdist(X0.T, metric=\"correlation\")\n",
    "        if np.isnan(row_dist).any():\n",
    "            row_dist = pdist(X0, metric=\"euclidean\")\n",
    "        if np.isnan(col_dist).any():\n",
    "            col_dist = pdist(X0.T, metric=\"euclidean\")\n",
    "\n",
    "        row_link = linkage(row_dist, method=\"average\")\n",
    "        col_link = linkage(col_dist, method=\"average\")\n",
    "        row_order = leaves_list(row_link)\n",
    "        col_order = leaves_list(col_link)\n",
    "\n",
    "        Jc = J.iloc[row_order, col_order]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(0.38 * Jc.shape[1] + 4.5, 0.34 * Jc.shape[0] + 4.0))\n",
    "        sns.heatmap(Jc, cmap=\"Reds\", vmin=0, vmax=np.nanmax(Jc.values), linewidths=0.2, linecolor=\"white\", ax=ax)\n",
    "        ax.set_title(\"Subtype mapping: scRNA vs Xenium (Jaccard)\")\n",
    "        ax.set_xlabel(\"Xenium subtype\")\n",
    "        ax.set_ylabel(\"scRNA subtype\")\n",
    "        fig.tight_layout()\n",
    "        heat_pdf = J_FIG / f\"{outstem}.heatmap.pdf\"\n",
    "        heat_png = J_FIG / f\"{outstem}.heatmap.png\"\n",
    "        fig.savefig(heat_pdf, bbox_inches=\"tight\")\n",
    "        fig.savefig(heat_png, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(f\"[OK] heatmap: {heat_pdf}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 9) Optional: Scanpy matrixplot of z-scored Jaccard (AI-friendly PDF)\n",
    "        # -------------------------\n",
    "        try:\n",
    "            adataJ = sc.AnnData(\n",
    "                X=Jc.values.astype(float),\n",
    "                obs=pd.DataFrame(index=Jc.index.astype(str)),\n",
    "                var=pd.DataFrame(index=Jc.columns.astype(str)),\n",
    "            )\n",
    "            adataJ.obs[\"subtype\"] = pd.Categorical(adataJ.obs_names, categories=adataJ.obs_names, ordered=True)\n",
    "\n",
    "            Xj = adataJ.X\n",
    "            Xz = (Xj - Xj.mean(axis=0, keepdims=True)) / (Xj.std(axis=0, ddof=0, keepdims=True) + 1e-9)\n",
    "            adataJ.layers[\"z\"] = Xz\n",
    "            _bak = adataJ.X.copy()\n",
    "            adataJ.X = adataJ.layers[\"z\"]\n",
    "\n",
    "            sc.tl.dendrogram(adataJ, groupby=\"subtype\")\n",
    "            v = np.nanpercentile(np.abs(adataJ.X), 98)\n",
    "\n",
    "            # save via object API to avoid scanpy version differences\n",
    "            mp = sc.pl.matrixplot(\n",
    "                adataJ,\n",
    "                var_names=adataJ.var_names,\n",
    "                groupby=\"subtype\",\n",
    "                dendrogram=True,\n",
    "                cmap=\"RdBu_r\",\n",
    "                vmin=-v, vmax=v,\n",
    "                colorbar_title=\"Z-scaled Jaccard\",\n",
    "                show=False,\n",
    "                return_fig=True,\n",
    "            )\n",
    "            mp_path = J_FIG / f\"{outstem}.matrixplot_zJaccard.pdf\"\n",
    "            mp.savefig(mp_path, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close(\"all\")\n",
    "\n",
    "            adataJ.X = _bak\n",
    "            print(f\"[OK] matrixplot: {mp_path}\")\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"[WARN] Scanpy matrixplot failed ({type(e).__name__}: {e}). Heatmap is still available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7778e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cellular neighborhoods (CN) from local cell-type composition\n",
    "# =========================\n",
    "if bool(CONFIG[\"RUN_CN\"]):\n",
    "    cn_cfg = CONFIG[\"CN\"]\n",
    "    cn_key_added = str(cn_cfg[\"key_added\"])\n",
    "    cn_obs_key = str(cn_cfg[\"cn_obs_key\"])\n",
    "\n",
    "    sq.gr.spatial_neighbors(\n",
    "        adata,\n",
    "        spatial_key=spatial_key,\n",
    "        library_key=sample_key,\n",
    "        coord_type=\"generic\",\n",
    "        delaunay=False,\n",
    "        radius=float(cn_cfg[\"radius_um\"]),\n",
    "        set_diag=True,\n",
    "        key_added=cn_key_added,\n",
    "    )\n",
    "\n",
    "    A = adata.obsp[f\"{cn_key_added}_connectivities\"].tocsr()\n",
    "    groups = adata.obs[subtype_key].astype(\"category\")\n",
    "    group_names = list(groups.cat.categories)\n",
    "\n",
    "    n_cells = adata.n_obs\n",
    "    n_groups = len(group_names)\n",
    "\n",
    "    codes = groups.cat.codes.to_numpy()\n",
    "    onehot = sparse.csr_matrix(\n",
    "        (np.ones(n_cells, dtype=np.float32), (np.arange(n_cells), codes)),\n",
    "        shape=(n_cells, n_groups),\n",
    "    )\n",
    "\n",
    "    window_counts = (A @ onehot).astype(np.float32)\n",
    "    row_sums = np.asarray(window_counts.sum(axis=1)).ravel()\n",
    "    row_sums[row_sums == 0] = 1.0\n",
    "    window_frac = window_counts.multiply(1.0 / row_sums[:, None]).toarray().astype(np.float32)\n",
    "\n",
    "    X = window_frac\n",
    "    if bool(cn_cfg[\"standardize_within_sample\"]):\n",
    "        Xz = np.zeros_like(X, dtype=np.float32)\n",
    "        for s in sorted(adata.obs[sample_key].astype(str).unique()):\n",
    "            idx = (adata.obs[sample_key].astype(str) == s).to_numpy()\n",
    "            if idx.sum() < 2:\n",
    "                continue\n",
    "            mu = X[idx].mean(axis=0, keepdims=True)\n",
    "            sd = X[idx].std(axis=0, ddof=0, keepdims=True)\n",
    "            sd[sd == 0] = 1.0\n",
    "            Xz[idx] = (X[idx] - mu) / sd\n",
    "        X = Xz\n",
    "\n",
    "    km = MiniBatchKMeans(\n",
    "        n_clusters=int(cn_cfg[\"n_clusters\"]),\n",
    "        random_state=SEED,\n",
    "        batch_size=4096,\n",
    "        n_init=10,\n",
    "    )\n",
    "    cn_labels = km.fit_predict(X)\n",
    "    adata.obs[cn_obs_key] = pd.Categorical([f\"CN{c:02d}\" for c in cn_labels])\n",
    "\n",
    "    comp_df = pd.DataFrame(window_frac, columns=group_names, index=adata.obs_names)\n",
    "    cn_means = comp_df.groupby(adata.obs[cn_obs_key]).mean()\n",
    "    overall = comp_df.mean(axis=0)\n",
    "    eps = 1e-6\n",
    "    log2fc = np.log2((cn_means + eps).div(overall + eps, axis=1))\n",
    "\n",
    "    cn_table_csv = TAB_DIR / \"cn_log2fc_vs_overall.csv\"\n",
    "    log2fc.to_csv(cn_table_csv, index=True)\n",
    "\n",
    "    cn_labels_csv = TAB_DIR / \"cn_labels_per_cell.csv\"\n",
    "    (\n",
    "        adata.obs[[sample_key, cell_id_key, cn_obs_key]]\n",
    "        .rename(columns={cn_obs_key: \"CN\"})\n",
    "        .to_csv(cn_labels_csv, index=False)\n",
    "    )\n",
    "\n",
    "    fig = plt.figure(figsize=(0.35 * log2fc.shape[1] + 6, 0.35 * log2fc.shape[0] + 4))\n",
    "    sns.heatmap(\n",
    "        log2fc,\n",
    "        cmap=\"coolwarm\",\n",
    "        center=0,\n",
    "        cbar_kws={\"label\": \"log2 fold-change vs overall\"},\n",
    "        linewidths=0.2,\n",
    "        linecolor=\"white\",\n",
    "    )\n",
    "    plt.title(f\"Cellular neighborhoods (radius={cn_cfg['radius_um']} µm)\")\n",
    "    plt.xlabel(\"Subtype\")\n",
    "    plt.ylabel(\"CN\")\n",
    "    plt.tight_layout()\n",
    "    cn_fig = FIG_DIR / \"cn_log2fc_heatmap.pdf\"\n",
    "    plt.savefig(cn_fig, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[OK] CN log2FC table: {cn_table_csv}\")\n",
    "    print(f\"[OK] CN labels: {cn_labels_csv}\")\n",
    "    print(f\"[OK] CN heatmap: {cn_fig}\")\n",
    "else:\n",
    "    print(\"[CN] Skipped (CONFIG['RUN_CN']=False)\")\n",
    "\n",
    "# =========================\n",
    "# Distance-to-target binning (query cells only)\n",
    "# =========================\n",
    "dist_cfg = CONFIG[\"DIST\"]\n",
    "subtype_str = adata.obs[subtype_key].astype(str)\n",
    "coords = np.asarray(adata.obsm[spatial_key])[:, :2]\n",
    "samples = adata.obs[sample_key].astype(str)\n",
    "cell_ids = adata.obs[cell_id_key].astype(str)\n",
    "\n",
    "# Pick the first target label that exists in the dataset\n",
    "target_label = next((t for t in dist_cfg[\"target_labels\"] if (subtype_str == t).any()), None)\n",
    "if target_label is None:\n",
    "    raise ValueError(\n",
    "        \"None of CONFIG['DIST']['target_labels'] were found in adata.obs['subtype']. \"\n",
    "        f\"Tried: {dist_cfg['target_labels']}\"\n",
    "    )\n",
    "print(f\"[DIST] target_label={target_label!r}\")\n",
    "\n",
    "# Query cells (regex on subtype)\n",
    "query_mask = pd.Series(False, index=adata.obs_names)\n",
    "for pat in dist_cfg[\"query_regex\"]:\n",
    "    query_mask |= subtype_str.str.contains(pat, regex=True, na=False)\n",
    "\n",
    "target_mask = (subtype_str == target_label)\n",
    "\n",
    "rows = []\n",
    "for s in sorted(samples.unique()):\n",
    "    idx_s = (samples == s).to_numpy()\n",
    "    idx_t = idx_s & target_mask.to_numpy()\n",
    "    idx_q = idx_s & query_mask.to_numpy()\n",
    "    if idx_t.sum() == 0 or idx_q.sum() == 0:\n",
    "        continue\n",
    "\n",
    "    tree = cKDTree(coords[idx_t])\n",
    "    try:\n",
    "        dists, _ = tree.query(coords[idx_q], k=1, workers=-1)\n",
    "    except TypeError:\n",
    "        dists, _ = tree.query(coords[idx_q], k=1)\n",
    "\n",
    "    rows.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                sample_key: s,\n",
    "                cell_id_key: cell_ids[idx_q].values,\n",
    "                \"query_subtype\": subtype_str[idx_q].values,\n",
    "                \"dist_um\": dists,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "if len(rows) == 0:\n",
    "    raise RuntimeError(\"No sample had both target cells and query cells; distance computation aborted.\")\n",
    "\n",
    "bin_df = pd.concat(rows, ignore_index=True)\n",
    "bin_df = bin_df[\n",
    "    (bin_df[\"dist_um\"] >= float(dist_cfg[\"distance_min_um\"]))\n",
    "    & (bin_df[\"dist_um\"] <= float(dist_cfg[\"distance_max_um\"]))\n",
    "].copy()\n",
    "\n",
    "# Create bins with labels: \"0-10\", \"11-20\", ...\n",
    "dmin = float(dist_cfg[\"distance_min_um\"])\n",
    "dmax = float(dist_cfg[\"distance_max_um\"])\n",
    "step = float(dist_cfg[\"bin_size_um\"])\n",
    "\n",
    "edges = np.arange(dmin, dmax + step, step)\n",
    "if edges[-1] < dmax:\n",
    "    edges = np.append(edges, dmax)\n",
    "\n",
    "if len(edges) < 2:\n",
    "    raise ValueError(\"Distance binning edges are invalid; check distance_min/max/bin_size.\")\n",
    "\n",
    "bin_labels = [f\"{int(edges[0])}-{int(edges[1])}\"]\n",
    "for i in range(1, len(edges) - 1):\n",
    "    start = int(edges[i] + 1)\n",
    "    end = int(edges[i + 1])\n",
    "    bin_labels.append(f\"{start}-{end}\")\n",
    "\n",
    "cats = pd.cut(bin_df[\"dist_um\"], bins=edges, right=True, include_lowest=True)\n",
    "interval_to_label = dict(zip(cats.cat.categories, bin_labels))\n",
    "bin_df[\"bin\"] = cats.map(interval_to_label)\n",
    "bin_df = bin_df.dropna(subset=[\"bin\"]).copy()\n",
    "\n",
    "dist_table_csv = TAB_DIR / \"distance_query_to_target_bins.csv\"\n",
    "bin_df.to_csv(dist_table_csv, index=False)\n",
    "print(f\"[OK] distance table: {dist_table_csv}\")\n",
    "\n",
    "# Add distance info back to adata.obs (query cells only; others remain NA)\n",
    "adata.obs[\"dist_um_to_target\"] = np.nan\n",
    "adata.obs[\"dist_bin_to_target\"] = pd.NA\n",
    "\n",
    "join_key = samples + \"||\" + cell_ids\n",
    "bin_key = bin_df[sample_key].astype(str) + \"||\" + bin_df[cell_id_key].astype(str)\n",
    "dist_map = pd.Series(bin_df[\"dist_um\"].values, index=bin_key)\n",
    "bin_map = pd.Series(bin_df[\"bin\"].values, index=bin_key)\n",
    "\n",
    "mapped_dist = join_key.map(dist_map)\n",
    "mapped_bin = join_key.map(bin_map)\n",
    "\n",
    "mask_q = mapped_dist.notna()\n",
    "adata.obs.loc[mask_q, \"dist_um_to_target\"] = mapped_dist.loc[mask_q].astype(float).values\n",
    "adata.obs.loc[mask_q, \"dist_bin_to_target\"] = pd.Categorical(mapped_bin.loc[mask_q].astype(str))\n",
    "\n",
    "adata.uns[\"distance_to_target\"] = {\n",
    "    \"target_label\": target_label,\n",
    "    \"query_regex\": dist_cfg[\"query_regex\"],\n",
    "    \"distance_min_um\": dmin,\n",
    "    \"distance_max_um\": dmax,\n",
    "    \"bin_size_um\": step,\n",
    "}\n",
    "\n",
    "# Example spatial visualization (one sample)\n",
    "ex_mask = (adata.obs[sample_key] == example_sample).to_numpy()\n",
    "ex_coords = coords[ex_mask]\n",
    "ex_sub = subtype_str[ex_mask]\n",
    "\n",
    "ex_target = (ex_sub == target_label).to_numpy()\n",
    "ex_query = pd.Series(ex_sub).str.contains(\"|\".join(dist_cfg[\"query_regex\"]), regex=True, na=False).to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5.5, 5.5))\n",
    "ax.scatter(ex_coords[ex_target, 0], ex_coords[ex_target, 1], s=2, alpha=0.8, label=f\"target: {target_label}\")\n",
    "ax.scatter(ex_coords[ex_query, 0], ex_coords[ex_query, 1], s=2, alpha=0.6, label=\"query (regex)\")\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_title(f\"{example_sample}: target vs query cells\")\n",
    "ax.set_xlabel(\"x (µm)\")\n",
    "ax.set_ylabel(\"y (µm)\")\n",
    "ax.legend(markerscale=4)\n",
    "fig.tight_layout()\n",
    "fig_path = FIG_DIR / \"example_sample_target_vs_query.pdf\"\n",
    "fig.savefig(fig_path, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "print(f\"[OK] example spatial plot: {fig_path}\")\n",
    "\n",
    "# =========================\n",
    "# Distance-binned composition and enrichment (query cells)\n",
    "# =========================\n",
    "alpha = float(dist_cfg[\"bayes_alpha\"])\n",
    "\n",
    "comp = (\n",
    "    bin_df\n",
    "    .groupby([\"bin\", \"query_subtype\"], observed=True)\n",
    "    .size()\n",
    "    .unstack(\"query_subtype\", fill_value=0)\n",
    ")\n",
    "\n",
    "def _bin_center(lbl: str) -> float:\n",
    "    a, b = lbl.split(\"-\")\n",
    "    return (float(a) + float(b)) / 2.0\n",
    "\n",
    "bin_order = sorted(comp.index.tolist(), key=_bin_center)\n",
    "comp = comp.reindex(bin_order)\n",
    "comp_frac = comp.div(comp.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "comp_by_sample = (\n",
    "    bin_df\n",
    "    .groupby([sample_key, \"bin\", \"query_subtype\"], observed=True)\n",
    "    .size()\n",
    "    .unstack(\"query_subtype\", fill_value=0)\n",
    "    .reindex(\n",
    "        pd.MultiIndex.from_product([sorted(adata.obs[sample_key].unique()), bin_order], names=[sample_key, \"bin\"]),\n",
    "        fill_value=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "out_prefix = \"query_to_target\"\n",
    "comp_csv = TAB_DIR / f\"{out_prefix}_composition_counts_over_bins.csv\"\n",
    "frac_csv = TAB_DIR / f\"{out_prefix}_composition_fraction_over_bins.csv\"\n",
    "comp_by_sample_csv = TAB_DIR / f\"{out_prefix}_composition_counts_by_sample_over_bins.csv\"\n",
    "comp.to_csv(comp_csv)\n",
    "comp_frac.to_csv(frac_csv)\n",
    "comp_by_sample.to_csv(comp_by_sample_csv)\n",
    "print(f\"[OK] {comp_csv}\")\n",
    "print(f\"[OK] {frac_csv}\")\n",
    "print(f\"[OK] {comp_by_sample_csv}\")\n",
    "\n",
    "# Figure: pooled stacked bar (fraction within each bin)\n",
    "subtype_order = comp.sum(axis=0).sort_values(ascending=False).index.tolist()\n",
    "frac_use = comp_frac[subtype_order]\n",
    "\n",
    "if len(subtype_order) <= 20:\n",
    "    colors = sns.color_palette(\"tab20\", n_colors=len(subtype_order))\n",
    "else:\n",
    "    colors = sns.color_palette(\"husl\", n_colors=len(subtype_order))\n",
    "palette = dict(zip(subtype_order, colors))\n",
    "\n",
    "fig_w = float(np.clip(0.8 * len(bin_order) + 4.0, 8.0, 22.0))\n",
    "fig, ax = plt.subplots(figsize=(fig_w, 4.8))\n",
    "bottom = np.zeros(len(bin_order), dtype=float)\n",
    "x = np.arange(len(bin_order))\n",
    "\n",
    "for st in subtype_order:\n",
    "    vals = frac_use[st].to_numpy()\n",
    "    ax.bar(x, vals, 0.82, bottom=bottom, label=st, color=palette[st], edgecolor=\"white\", linewidth=0.3)\n",
    "    bottom += vals\n",
    "\n",
    "n_total = comp.sum(axis=1).to_numpy()\n",
    "for i, n in enumerate(n_total):\n",
    "    ax.text(x[i], 1.01, f\"n={int(n)}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(bin_order, rotation=0)\n",
    "ax.set_ylabel(\"Fraction within bin\")\n",
    "ax.set_xlabel(f\"Distance to nearest {target_label} (µm)\")\n",
    "ax.set_ylim(0, 1.08)\n",
    "\n",
    "legend_cols = min(6, max(3, int(np.ceil(len(subtype_order) / 2))))\n",
    "ax.legend(ncol=legend_cols, bbox_to_anchor=(0.5, -0.18), loc=\"upper center\", title=\"Query subtype\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig_a = FIG_DIR / f\"A_{out_prefix}_stacked_fraction.pdf\"\n",
    "fig.savefig(fig_a, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "print(f\"[OK] {fig_a}\")\n",
    "\n",
    "# Figure: log2 enrichment vs sample baseline (bubble heatmap)\n",
    "total_by_sample_subtype = comp_by_sample.groupby(level=sample_key).sum()\n",
    "p_base = total_by_sample_subtype.div(total_by_sample_subtype.sum(axis=1), axis=0).replace(0, np.nan)\n",
    "\n",
    "n_sb = comp_by_sample.sum(axis=1)  # total query cells per (sample, bin)\n",
    "long_counts = comp_by_sample.stack().rename(\"y\").reset_index()\n",
    "long_totals = n_sb.rename(\"n\").reset_index()\n",
    "long = long_counts.merge(long_totals, on=[sample_key, \"bin\"], how=\"left\")\n",
    "\n",
    "p_base_long = p_base.stack().rename(\"p_base\").reset_index()\n",
    "p_base_long = p_base_long.rename(columns={0: \"p_base\", \"level_1\": \"query_subtype\"})\n",
    "long = long.merge(p_base_long, on=[sample_key, \"query_subtype\"], how=\"left\")\n",
    "\n",
    "long[\"p_hat\"] = (long[\"y\"] + alpha) / (long[\"n\"] + 2 * alpha)\n",
    "long[\"log2_enrich_vs_sample\"] = np.log2((long[\"p_hat\"] / long[\"p_base\"]).replace(0, np.nan))\n",
    "\n",
    "agg = long.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"log2_enrich_vs_sample\"])\n",
    "enrich_mean = agg.groupby([\"bin\", \"query_subtype\"])[\"log2_enrich_vs_sample\"].mean().unstack(fill_value=0)\n",
    "count_sum = agg.groupby([\"bin\", \"query_subtype\"])[\"y\"].sum().unstack(fill_value=0)\n",
    "\n",
    "enrich_mean.to_csv(TAB_DIR / f\"{out_prefix}_log2_enrichment_vs_sample_baseline_mean.csv\")\n",
    "count_sum.to_csv(TAB_DIR / f\"{out_prefix}_counts_for_enrichment_heatmap.csv\")\n",
    "\n",
    "bins = enrich_mean.index.tolist()\n",
    "subtypes = enrich_mean.columns.tolist()\n",
    "\n",
    "Xp, Yp, Cc, Ss = [], [], [], []\n",
    "for i, b in enumerate(bins):\n",
    "    for j, st in enumerate(subtypes):\n",
    "        Xp.append(i)\n",
    "        Yp.append(j)\n",
    "        Cc.append(enrich_mean.loc[b, st])\n",
    "        Ss.append(np.log10(count_sum.loc[b, st] + 1.0) * 60.0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(0.55 * len(bins) + 4, 0.35 * len(subtypes) + 3))\n",
    "sca = ax.scatter(Xp, Yp, c=Cc, s=Ss, cmap=\"coolwarm\", vmin=-2, vmax=2, edgecolors=\"k\", linewidths=0.2)\n",
    "ax.set_xticks(range(len(bins)))\n",
    "ax.set_xticklabels(bins, rotation=0)\n",
    "ax.set_yticks(range(len(subtypes)))\n",
    "ax.set_yticklabels(subtypes)\n",
    "ax.set_xlabel(f\"Distance to nearest {target_label} (µm)\")\n",
    "ax.set_ylabel(\"Query subtype\")\n",
    "cb = fig.colorbar(sca, ax=ax, shrink=0.8)\n",
    "cb.set_label(\"Mean log2 enrichment vs sample baseline\")\n",
    "fig.tight_layout()\n",
    "fig_c = FIG_DIR / f\"C_{out_prefix}_bubble_enrichment_vs_sample.pdf\"\n",
    "fig.savefig(fig_c, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "print(f\"[OK] {fig_c}\")\n",
    "\n",
    "# =========================\n",
    "# Gene/program gradients along distance bins (query cells)\n",
    "# =========================\n",
    "gg_cfg = CONFIG[\"GENE_GRADIENT\"]\n",
    "\n",
    "if adata.raw is None:\n",
    "    raise RuntimeError(\"adata.raw is required for gene/program gradients (full-gene log1p matrix).\")\n",
    "\n",
    "raw_var = adata.raw.var_names\n",
    "raw_lookup = _uppercase_lookup(raw_var)\n",
    "\n",
    "X_expr = adata.raw.X\n",
    "q_mask = adata.obs[\"dist_bin_to_target\"].notna().to_numpy()\n",
    "if q_mask.sum() == 0:\n",
    "    raise RuntimeError(\"No query cells have distance bins assigned; cannot run gene/program gradients.\")\n",
    "\n",
    "q_idx = np.where(q_mask)[0]\n",
    "q_meta = adata.obs.loc[q_mask, [sample_key, cell_id_key, subtype_key, \"dist_bin_to_target\"]].copy()\n",
    "q_meta = q_meta.rename(columns={\"dist_bin_to_target\": \"bin\"})\n",
    "q_meta[\"bin\"] = q_meta[\"bin\"].astype(str)\n",
    "q_meta = q_meta.reset_index(drop=True)\n",
    "q_meta[\"_row_ix\"] = q_idx\n",
    "\n",
    "# ---- 1) Selected genes: mean expression vs distance bin ----\n",
    "genes_u = [g.upper() for g in gg_cfg[\"genes\"]]\n",
    "genes_present = [raw_lookup[g] for g in genes_u if g in raw_lookup]\n",
    "missing_genes = sorted(set(genes_u) - set(raw_lookup.keys()))\n",
    "if missing_genes:\n",
    "    warnings.warn(f\"[WARN] missing genes skipped: {missing_genes}\")\n",
    "\n",
    "if len(genes_present) > 0:\n",
    "    gene_ix = [raw_var.get_loc(g) for g in genes_present]\n",
    "    Xg = X_expr[:, gene_ix]\n",
    "\n",
    "    rows = []\n",
    "    for (s, b), df_g in q_meta.groupby([sample_key, \"bin\"], observed=True):\n",
    "        if len(df_g) < int(gg_cfg[\"min_cells_per_sample_bin\"]):\n",
    "            continue\n",
    "        ix = df_g[\"_row_ix\"].to_numpy()\n",
    "        mu = np.asarray(Xg[ix].mean(axis=0)).ravel()\n",
    "        rows.append(pd.Series(mu, index=genes_present, name=(str(s), str(b))))\n",
    "\n",
    "    if len(rows) > 0:\n",
    "        gene_means = pd.DataFrame(rows)\n",
    "        gene_means.index = pd.MultiIndex.from_tuples(gene_means.index, names=[sample_key, \"bin\"])\n",
    "\n",
    "        gene_means_csv = TAB_DIR / \"selected_genes_mean_expression_by_sample_bin.csv\"\n",
    "        gene_means.to_csv(gene_means_csv)\n",
    "\n",
    "        mean_by_bin = gene_means.groupby(level=\"bin\").mean().reindex(bin_order)\n",
    "        sem_by_bin = gene_means.groupby(level=\"bin\").sem(ddof=1).reindex(bin_order)\n",
    "\n",
    "        x = np.arange(len(mean_by_bin.index))\n",
    "        fig, ax = plt.subplots(figsize=(8.2, 4.6))\n",
    "        for g in genes_present:\n",
    "            y = mean_by_bin[g].to_numpy()\n",
    "            ax.plot(x, y, label=g)\n",
    "            lo = y - 2 * sem_by_bin[g].to_numpy()\n",
    "            hi = y + 2 * sem_by_bin[g].to_numpy()\n",
    "            ax.fill_between(x, lo, hi, alpha=0.15)\n",
    "\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(mean_by_bin.index.tolist(), rotation=0)\n",
    "        ax.set_xlabel(f\"Distance to nearest {target_label} (µm)\")\n",
    "        ax.set_ylabel(\"Mean log1p-normalized expression (±2 SEM)\")\n",
    "        ax.legend(ncol=2)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig_path = FIG_DIR / \"G_selected_genes_mean_vs_distance.pdf\"\n",
    "        fig.savefig(fig_path, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"[OK] {gene_means_csv}\")\n",
    "        print(f\"[OK] {fig_path}\")\n",
    "    else:\n",
    "        warnings.warn(\"[WARN] no (sample, bin) groups passed min_cells_per_sample_bin; skipping selected-gene summaries.\")\n",
    "else:\n",
    "    warnings.warn(\"[WARN] none of the configured genes are present; skipping selected-gene plots.\")\n",
    "\n",
    "# ---- 2) Signature scores (Scanpy score_genes) ----\n",
    "sig_scores = []\n",
    "for name, gene_list in gg_cfg[\"signatures\"].items():\n",
    "    present = []\n",
    "    for g in gene_list:\n",
    "        gu = str(g).upper()\n",
    "        if gu in raw_lookup:\n",
    "            present.append(raw_lookup[gu])\n",
    "    present = list(dict.fromkeys(present))  # keep order, drop duplicates\n",
    "\n",
    "    if len(present) < 5:\n",
    "        warnings.warn(f\"[WARN] signature {name!r}: too few genes present ({len(present)}); skipped.\")\n",
    "        continue\n",
    "\n",
    "    score_name = f\"sig_{name}\"\n",
    "    sc.tl.score_genes(\n",
    "        adata,\n",
    "        gene_list=present,\n",
    "        score_name=score_name,\n",
    "        ctrl_size=int(gg_cfg[\"score_ctrl_size\"]),\n",
    "        n_bins=int(gg_cfg[\"score_n_bins\"]),\n",
    "        random_state=SEED,\n",
    "        use_raw=True,\n",
    "    )\n",
    "    sig_scores.append(score_name)\n",
    "\n",
    "if len(sig_scores) > 0:\n",
    "    score_meta = adata.obs.loc[q_mask, [sample_key, \"dist_bin_to_target\"] + sig_scores].copy()\n",
    "    score_meta = score_meta.rename(columns={\"dist_bin_to_target\": \"bin\"})\n",
    "    score_meta[\"bin\"] = score_meta[\"bin\"].astype(str)\n",
    "\n",
    "    score_means = score_meta.groupby([sample_key, \"bin\"], observed=True)[sig_scores].mean()\n",
    "    score_means = score_means.reindex(\n",
    "        pd.MultiIndex.from_product([sorted(adata.obs[sample_key].unique()), bin_order], names=[sample_key, \"bin\"])\n",
    "    )\n",
    "\n",
    "    score_means_csv = TAB_DIR / \"signature_scores_mean_by_sample_bin.csv\"\n",
    "    score_means.to_csv(score_means_csv)\n",
    "\n",
    "    mean_by_bin = score_means.groupby(level=\"bin\").mean().reindex(bin_order)\n",
    "    sem_by_bin = score_means.groupby(level=\"bin\").sem(ddof=1).reindex(bin_order)\n",
    "\n",
    "    x = np.arange(len(mean_by_bin.index))\n",
    "    fig, ax = plt.subplots(figsize=(8.2, 4.6))\n",
    "    for sc_name in sig_scores:\n",
    "        y = mean_by_bin[sc_name].to_numpy()\n",
    "        ax.plot(x, y, label=sc_name.replace(\"sig_\", \"\"))\n",
    "        lo = y - 2 * sem_by_bin[sc_name].to_numpy()\n",
    "        hi = y + 2 * sem_by_bin[sc_name].to_numpy()\n",
    "        ax.fill_between(x, lo, hi, alpha=0.15)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(mean_by_bin.index.tolist(), rotation=0)\n",
    "    ax.set_xlabel(f\"Distance to nearest {target_label} (µm)\")\n",
    "    ax.set_ylabel(\"Mean signature score (±2 SEM)\")\n",
    "    ax.legend(ncol=2)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig_path = FIG_DIR / \"G_signature_scores_mean_vs_distance.pdf\"\n",
    "    fig.savefig(fig_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"[OK] {score_means_csv}\")\n",
    "    print(f\"[OK] {fig_path}\")\n",
    "else:\n",
    "    print(\"[SIG] No signature scores were computed (no gene sets passed the presence threshold).\")\n",
    "\n",
    "# ---- 3) Rank genes by monotonic gradient across bins ----\n",
    "def rank_gradient_genes(\n",
    "    adata: \"sc.AnnData\",\n",
    "    *,\n",
    "    q_mask: np.ndarray,\n",
    "    bin_labels: list,\n",
    "    sample_key: str,\n",
    "    min_cells_per_group: int,\n",
    "    smooth_min: float,\n",
    "    r2_min: float,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Return per-gene gradient metrics and a filtered table.\"\"\"\n",
    "    X = adata.raw.X if adata.raw is not None else adata.X\n",
    "    var_names = adata.raw.var_names if adata.raw is not None else adata.var_names\n",
    "\n",
    "    q_idx = np.where(q_mask)[0]\n",
    "    meta = adata.obs.loc[q_mask, [sample_key, \"dist_bin_to_target\"]].copy()\n",
    "    meta = meta.rename(columns={\"dist_bin_to_target\": \"bin\"})\n",
    "    meta[\"bin\"] = meta[\"bin\"].astype(str)\n",
    "    meta[\"_row_ix\"] = q_idx\n",
    "\n",
    "    centers = np.array([_bin_center(b) for b in bin_labels], dtype=float)\n",
    "\n",
    "    group_rows = []\n",
    "    group_index = []\n",
    "    for (s, b), df_g in meta.groupby([sample_key, \"bin\"], observed=True):\n",
    "        if len(df_g) < min_cells_per_group:\n",
    "            continue\n",
    "        ix = df_g[\"_row_ix\"].to_numpy()\n",
    "        mu = np.asarray(X[ix].mean(axis=0)).ravel()\n",
    "        group_rows.append(mu)\n",
    "        group_index.append((str(s), str(b)))\n",
    "\n",
    "    if len(group_rows) == 0:\n",
    "        raise RuntimeError(\"No (sample, bin) groups passed min_cells_per_group; cannot rank genes.\")\n",
    "\n",
    "    M = np.vstack(group_rows)\n",
    "    group_index = pd.MultiIndex.from_tuples(group_index, names=[sample_key, \"bin\"])\n",
    "    mean_df = pd.DataFrame(M, index=group_index, columns=var_names)\n",
    "\n",
    "    bin_means = mean_df.groupby(level=\"bin\").mean().reindex(bin_labels)\n",
    "    Y = bin_means.to_numpy()\n",
    "\n",
    "    dy = np.abs(np.diff(Y, axis=0))\n",
    "    y_range = (Y.max(axis=0) - Y.min(axis=0)) + 1e-9\n",
    "    smooth = 1.0 - (dy.mean(axis=0) / y_range)\n",
    "\n",
    "    r = np.array([spearmanr(centers, Y[:, j]).correlation for j in range(Y.shape[1])])\n",
    "    r = np.nan_to_num(r, nan=0.0)\n",
    "    r2 = r ** 2\n",
    "\n",
    "    x0 = centers - centers.mean()\n",
    "    var_x = (x0 ** 2).sum()\n",
    "    slope = (x0[:, None] * (Y - Y.mean(axis=0)[None, :])).sum(axis=0) / var_x\n",
    "\n",
    "    metrics = pd.DataFrame(\n",
    "        {\n",
    "            \"gene\": var_names,\n",
    "            \"smooth\": smooth,\n",
    "            \"spearman_r\": r,\n",
    "            \"spearman_r2\": r2,\n",
    "            \"slope\": slope,\n",
    "        }\n",
    "    ).sort_values(\"slope\", ascending=False)\n",
    "\n",
    "    keep = (metrics[\"smooth\"] >= smooth_min) & (metrics[\"spearman_r2\"] >= r2_min)\n",
    "    return metrics, metrics.loc[keep].copy()\n",
    "\n",
    "metrics_all, metrics_filt = rank_gradient_genes(\n",
    "    adata,\n",
    "    q_mask=q_mask,\n",
    "    bin_labels=bin_order,\n",
    "    sample_key=sample_key,\n",
    "    min_cells_per_group=int(gg_cfg[\"min_cells_per_sample_bin\"]),\n",
    "    smooth_min=float(gg_cfg[\"smooth_min\"]),\n",
    "    r2_min=float(gg_cfg[\"r2_min\"]),\n",
    ")\n",
    "\n",
    "metrics_csv = TAB_DIR / \"gene_gradient_metrics.csv\"\n",
    "metrics_all.to_csv(metrics_csv, index=False)\n",
    "\n",
    "top_k = int(gg_cfg[\"top_k\"])\n",
    "top_pos = metrics_filt.sort_values(\"slope\", ascending=False).head(top_k)\n",
    "top_neg = metrics_filt.sort_values(\"slope\", ascending=True).head(top_k)\n",
    "top_tbl = pd.concat([top_pos, top_neg], axis=0, ignore_index=True)\n",
    "\n",
    "top_csv = TAB_DIR / f\"ranked_genes_top{top_k}pos_top{top_k}neg.csv\"\n",
    "top_tbl.to_csv(top_csv, index=False)\n",
    "\n",
    "print(f\"[OK] {metrics_csv}\")\n",
    "print(f\"[OK] {top_csv}\")\n",
    "\n",
    "\n",
    "# ---- Save final object ----\n",
    "final_h5ad = DIRS[\"adata\"] / \"xenium_integrated.h5ad\"\n",
    "adata.write_h5ad(final_h5ad)\n",
    "print(f\"[OK] wrote: {final_h5ad}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
